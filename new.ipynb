{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# My First Natural Language Processor\n",
    "\n",
    "CSCI 1360: Foundations for Informatics and Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Overview and Objectives\n",
    "\n",
    "Last week, we introduced the concept of natural language processing, and in particular the \"bag of words\" model for representing and quantifying text for later analysis. In this lecture, we'll expand on those topics, including some additional preprocessing and text representation methods. By the end of this lecture, you should be able to\n",
    " - Implement several preprocessing techniques like stemming, stopwords, and minimum counts\n",
    " - Understand the concept of *feature vectors* in natural language processing\n",
    " - Compute inverse document frequencies to up or down-weight term frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some preprocessing techniques with text we've covered in the last lecture!\n",
    " - Lower case (or upper case) everything\n",
    " - Split into single words\n",
    " - Remove trailing whitespace (spaces, tabs, newlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To start, let's download a few more books in addition to *Alice in Wonderland* example from the previous lecture. Links are below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " - [*Alice in Wonderland*](https://www.gutenberg.org/ebooks/11), by Lewis Carroll\n",
    " - [*Pride and Prejudice*](https://www.gutenberg.org/ebooks/1342), by Jane Austen\n",
    " - [*Frankenstein*](https://www.gutenberg.org/ebooks/84), by Mary Shelley\n",
    " - [*Beowulf*](https://www.gutenberg.org/ebooks/16328), by Lesslie Hall\n",
    " - [*The Adventures of Sherlock Holmes*](https://www.gutenberg.org/ebooks/1661), by Sir Arthur Conan Doyle\n",
    " - [*The Adventures of Tom Sawyer*](https://www.gutenberg.org/ebooks/74), by Mark Twain\n",
    " - [*The Adventures of Huckleberry Finn*](https://www.gutenberg.org/ebooks/76), by Mark Twain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, we'll read all the books' raw contents into a dictionary. We use a dictionary to store all the text from the text books above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "books = {}  # We'll use a dictionary to store all the text from the books.\n",
    "files = ['NLP-ICA/alice.txt',\n",
    "         'NLP-ICA/pride.txt',\n",
    "         'NLP-ICA/frank.txt',\n",
    "         'NLP-ICA/bwulf.txt',\n",
    "         'NLP-ICA/holmes.txt',\n",
    "         'NLP-ICA/tom.txt',\n",
    "         'NLP-ICA/finn.txt']\n",
    "\n",
    "for f in files:\n",
    "    # This weird line just takes the part of the filename between the \"/\" and \".\" as the dict key.\n",
    "    prefix = f.split(\"/\")[-1].split(\".\")[0]\n",
    "    try:\n",
    "        with open(f, \"r\", encoding = \"ISO-8859-1\") as descriptor:\n",
    "            books[prefix] = descriptor.read()\n",
    "    except:\n",
    "        print(\"File '{}' had an error!\".format(f))\n",
    "        books[prefix] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['alice', 'pride', 'frank', 'bwulf', 'holmes', 'tom', 'finn'])\n"
     ]
    }
   ],
   "source": [
    "# Here you can see the dict keys (i.e. the results of the weird line of code in the last cell)\n",
    "print(books.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's go ahead and lower case everything, strip out whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(book):\n",
    "    # First, lowercase everything.\n",
    "    lower=book.lower()\n",
    "    \n",
    "    # Second, split into lines.\n",
    "    lines=lower.split(\"\\n\")\n",
    "    \n",
    "    # Third, split each line into words.\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        words.extend(line.strip().split(\" \"))\n",
    "   \n",
    "    \n",
    "\n",
    "    # That's it!\n",
    "    return count(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will count all the words. This function takes a list of words as input, and counts them all up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict # Our good friend from the last lecture, defaultdict!\n",
    "\n",
    "\n",
    "def count(words):\n",
    "    counts=defaultdict(int)\n",
    "    \n",
    "    for word in words:\n",
    "        counts[word]+=1\n",
    "        \n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's loop through our books and count all the words that show up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for k, v in books.items():\n",
    "    counts[k] = preprocess(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how our basic preprocessing techniques worked out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'alice': 5584 unique words; most common word 'the' appeared 1777 times.\n",
      "'pride': 13127 unique words; most common word 'the' appeared 4479 times.\n",
      "'frank': 11725 unique words; most common word 'the' appeared 4327 times.\n",
      "'bwulf': 11024 unique words; most common word '' appeared 3497 times.\n",
      "'holmes': 14544 unique words; most common word 'the' appeared 5704 times.\n",
      "'tom': 13449 unique words; most common word 'the' appeared 3907 times.\n",
      "'finn': 13840 unique words; most common word 'and' appeared 6107 times.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_results(counts):\n",
    "    for key, bag_of_words in counts.items():\n",
    "        word_counts = Counter(bag_of_words) # Remember \"Counter\"?\n",
    "        mc_word, mc_count = word_counts.most_common(1)[0]\n",
    "        print(\"'{}': {} unique words; most common word '{}' appeared {} times.\"\n",
    "              .format(key, len(bag_of_words.keys()), mc_word, mc_count))\n",
    "print_results(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's take a quick step back and think about the code we just saw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - The **`preprocess`** function takes a single book string as input and does some preprocessing: it lowercases everything so it's all the same case, it splits up the string into single words, and it adds all these words to one big list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - We also have a **`count`** function, which takes a list of words (output from `preprocess`) and counts everything up into a dictionary (the keys are unique words, the values how many times those words appear in the book). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " - Finally, we have a block of code that loops over all our books and runs these two functions on each of them, building dictionaries of word counts. These are fed into **`print_results`** so that we can see 1) the number of unique words in each book, and 2) the most common word in each book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We're going to repeat this process for the rest of this activity, slowly upgrading the **`preprocess`** function so that the final result (top words for each book) become more meaningful and indicative of the books' contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A great first step is to implement stop words. (You can use [this list of 319 stop words](http://xpo6.com/list-of-english-stop-words/)). This code just reads in the words from a stoplist file and adds them to a list we can use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst', 'amount', 'an', 'and', 'another', 'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'around', 'as', 'at', 'back', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'bill', 'both', 'bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant', 'co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de', 'describe', 'detail', 'do', 'done', 'down', 'due', 'during', 'each', 'eg', 'eight', 'either', 'eleven', 'else', 'elsewhere', 'empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herse\"', 'him', 'himse\"', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itse\"', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myse\"', 'name', 'namely', 'neither', 'never', 'nevertheless', 'next', 'nine', 'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of', 'off', 'often', 'on', 'once', 'one', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'part', 'per', 'perhaps', 'please', 'put', 'rather', 're', 'same', 'see', 'seem', 'seemed', 'seeming', 'seems', 'serious', 'several', 'she', 'should', 'show', 'side', 'since', 'sincere', 'six', 'sixty', 'so', 'some', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhere', 'still', 'such', 'system', 'take', 'ten', 'than', 'that', 'the', 'their', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'thick', 'thin', 'third', 'this', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'top', 'toward', 'towards', 'twelve', 'twenty', 'two', 'un', 'under', 'until', 'up', 'upon', 'us', 'very', 'via', 'was', 'we', 'well', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with', 'within', 'without', 'would', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "with open(\"NLP-ICA/stopwords.txt\", \"r\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    stopwords = [w.strip() for w in lines]\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use the words in the `stopwords` list to eliminate words from our books (remember: we consider \"stop words\" to be meaningless to the overall semantics of the text; go back to the previous lecture if you need a refresher on stop words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we'll augment our `preprocess` function to include stop word processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_v2(book, stopwords):  # Note the \"_v2\"--this is a new function!\n",
    "    # First, lowercase everything.\n",
    "    \n",
    "    lower=book.lower()\n",
    "    \n",
    "    # Second, split into lines.\n",
    "    lines=lower.split(\"\\n\")\n",
    "    \n",
    "    # Third, split each line into words.\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        words.extend(line.strip().split(\" \"))\n",
    "           ### NEW CODE HERE! Check for stopwords.\n",
    "    new_words=[]\n",
    "    for word in words:\n",
    "        if word in stopwords:\n",
    "            pass\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    # That's it!\n",
    "    return count(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's see what we have! Same code as before--count the words in the books, but this time the stopwords will be filtered out per the new code in the preprocess_v2() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'alice': 5350 unique words; most common word '' appeared 1178 times.\n",
      "'pride': 12857 unique words; most common word '' appeared 2474 times.\n",
      "'frank': 11463 unique words; most common word '' appeared 2841 times.\n",
      "'bwulf': 10781 unique words; most common word '' appeared 3497 times.\n",
      "'holmes': 14278 unique words; most common word '' appeared 2750 times.\n",
      "'tom': 13180 unique words; most common word '' appeared 2283 times.\n",
      "'finn': 13590 unique words; most common word '' appeared 2719 times.\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "for k, v in books.items():\n",
    "    counts[k] = preprocess_v2(v, stopwords)\n",
    "    \n",
    "print_results(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Errr... this seems even worse! What's with the `''` word?! What could we try next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minimum length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our problem with \"words\" that are just blank strings `''` is that \n",
    " 1. Text is weird, and \n",
    " 2. We haven't told Python that we don't care about super-short \"words\" (that aren't really words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get around this by adding another filtering step in our preprocess function: drop any word that has fewer than a certain number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Put another way: **ignore words under a certain length.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's another new preprocessing function, this time enforcing a minimum word length of 3 (anything less than that is ignored the same way we ignore stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_v3(book, stopwords):  # We've reached \"_v3\"!\n",
    "    # First, lowercase everything.\n",
    "    \n",
    "    low=book.lower()\n",
    "    \n",
    "    # Second, split into lines.\n",
    "    lines=low.split(\"\\n\")\n",
    "    \n",
    "    # Third, split each line into words.\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        words.extend(line.strip().split(\" \"))\n",
    "           # Skip stopwords OR words with length under 2\n",
    "    new_words=[]\n",
    "    tokens=[]\n",
    "    for word in words:\n",
    "        if word in stopwords:\n",
    "            pass\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    for word in new_words:\n",
    "        if len(word) < 2:\n",
    "            pass\n",
    "        else:\n",
    "            tokens.append(word)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # That's it!\n",
    "    return count(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Maybe this will be better? Same word-counting again, this time using preprocess_v3 to enforce both stopwords and minimum word length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'alice': 5344 unique words; most common word 'said' appeared 421 times.\n",
      "'pride': 12845 unique words; most common word 'mr.' appeared 766 times.\n",
      "'frank': 11451 unique words; most common word 'me,' appeared 148 times.\n",
      "'bwulf': 10761 unique words; most common word 'beowulf' appeared 112 times.\n",
      "'holmes': 14267 unique words; most common word 'said' appeared 448 times.\n",
      "'tom': 13170 unique words; most common word 'tom' appeared 461 times.\n",
      "'finn': 13581 unique words; most common word 'got' appeared 603 times.\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "for k, v in books.items():\n",
    "    counts[k] = preprocess_v3(v, stopwords)\n",
    "    \n",
    "print_results(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Ooh! Definite improvement! Though clearly, punctuation is getting in the way; you can see it in at least two of the top words in the list above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We spoke last time about how removing punctuation could be a little dangerous; what if the punctuation is inherent to the meaning of the word (i.e., a contraction)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In our fourth version of the preprocess function, we'll compromise a little: we'll get the \"easy\" punctuation, like exclamation marks, periods, and commas, and leave the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_v4(book, stopwords):\n",
    "\n",
    "    # First, lowercase everything.\n",
    "    \n",
    "    low=book.lower()\n",
    "    \n",
    "    # Second, split into lines.\n",
    "    lines=low.split(\"\\n\")\n",
    "    \n",
    "    # Third, split each line into words.\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        words.extend(line.strip().split(\" \"))\n",
    "           # Skip stopwords OR words with length under 2\n",
    "    new_words=[]\n",
    "    tokens=[]\n",
    "    for word in words:\n",
    "        if word in stopwords:\n",
    "            pass\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    for word in new_words:\n",
    "        if len(word) < 2:\n",
    "            pass\n",
    "        else:\n",
    "            tokens.append(word)\n",
    "\n",
    "            ### NEW CODE HERE! Cut off any end-of-sentence punctuation. Use endswith() function\n",
    "    new_tokens=[]\n",
    "    for word in tokens:\n",
    "\n",
    "        if word.endswith(\"!\") or word.endswith(\".\") or word.endswith(\",\") or word.endswith(\":\") or word.endswith(\";\") or word.endswith(\"?\"):\n",
    "    \n",
    "            new_tokens.append(word[:-1])\n",
    "        else:\n",
    "            new_tokens.append(word)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # That's it!\n",
    "    return count(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alright, let's check it out again. Same thing AGAIN, this time with preprocess_v4 to know out commas, exclamation marks, periods, colons, and semicolons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'alice': 4181 unique words; most common word 'said' appeared 456 times.\n",
      "'pride': 8544 unique words; most common word 'mr' appeared 766 times.\n",
      "'frank': 7960 unique words; most common word 'me' appeared 324 times.\n",
      "'bwulf': 8736 unique words; most common word 'beowulf' appeared 156 times.\n",
      "'holmes': 10781 unique words; most common word 'said' appeared 484 times.\n",
      "'tom': 10038 unique words; most common word 'tom' appeared 611 times.\n",
      "'finn': 9976 unique words; most common word 'says' appeared 628 times.\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "for k, v in books.items():\n",
    "    counts[k] = preprocess_v4(v, stopwords)\n",
    "    \n",
    "print_results(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now we're getting somewhere! But this introduces a new concept--in looking at this list, wouldn't you say that \"says\" and \"said\" are probably, semantically, more or less the same word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Stemming is the process by which we convert words with similar meaning into the same word, so their similarity is reflected in our analysis. Words like \"imaging\" and \"images\", or \"says\" and \"said\" should probably be considered the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To do this, we'll need an external Python package: the Natural Language Toolkit, or NLTK. (it's installed on JupyterHub, so go ahead and play with it!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NLTK](https://www.nltk.org/) has a ton of really interesting stuff that goes way beyond the scope of this lecture. For our purposes, though, we're just going to use its \"stemming\" capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk # This package!\n",
    "\n",
    "def preprocess_v5(book, stopwords):\n",
    "    stemmer=nltk.stem.SnowballStemmer('english')\n",
    "    \n",
    "    # First, lowercase everything.\n",
    "    \n",
    "    low=book.lower()\n",
    "    \n",
    "    # Second, split into lines.\n",
    "    lines=low.split(\"\\n\")\n",
    "    \n",
    "    # Third, split each line into words.\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        words.extend(line.strip().split(\" \"))\n",
    "           # Skip stopwords OR words with length under 2\n",
    "    new_words=[]\n",
    "    tokens=[]\n",
    "    for word in words:\n",
    "        if word in stopwords:\n",
    "            pass\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    for word in new_words:\n",
    "        if len(word) < 2:\n",
    "            pass\n",
    "        else:\n",
    "            tokens.append(word)\n",
    "\n",
    "            ### NEW CODE HERE! Cut off any end-of-sentence punctuation. Use endswith() function\n",
    "    new_tokens=[]\n",
    "    stem_words=[]\n",
    "\n",
    "    for word in tokens:\n",
    "\n",
    "        if word.endswith(\"!\") or word.endswith(\".\") or word.endswith(\",\") or word.endswith(\":\") or word.endswith(\";\") or word.endswith(\"?\"):\n",
    "    \n",
    "            new_tokens.append(word[:-1])\n",
    "        else:\n",
    "            new_tokens.append(word)\n",
    "        \n",
    "    \n",
    "    \n",
    "            ### NEW CODE HERE! Just this next line (though\n",
    "            ### it's initialized at the start of the function)\n",
    "    \n",
    "    for word in new_tokens:\n",
    "        stemmed = stemmer.stem(word)  # This is all that is required--nltk does the rest!\n",
    "        stem_words.append(stemmed)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "    # That's it!\n",
    "    return count(stem_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How did this go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'alice': 3489 unique words; most common word 'said' appeared 456 times.\n",
      "'pride': 5975 unique words; most common word 'mr' appeared 767 times.\n",
      "'frank': 5516 unique words; most common word 'me' appeared 324 times.\n",
      "'bwulf': 7072 unique words; most common word 'beowulf' appeared 192 times.\n",
      "'holmes': 8197 unique words; most common word 'said' appeared 484 times.\n",
      "'tom': 7628 unique words; most common word 'tom' appeared 705 times.\n",
      "'finn': 8017 unique words; most common word 'say' appeared 847 times.\n"
     ]
    }
   ],
   "source": [
    "# Up to version 5 to enforce word stemming!\n",
    "counts = {}\n",
    "for k, v in books.items():\n",
    "    counts[k] = preprocess_v5(v, stopwords)\n",
    "    \n",
    "print_results(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Well, this *kinda* helped--\"says\" was reduced to \"say\", and its count clearly increased from the 628 it was before, meaning stemmed versions that were previously viewed as different words were merged. But \"said\" is still there; clearly, there are limitations to this stemmer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As one final step--it's convenient sometimes to simply drop words that occur only once or twice. This can dramatically help with processing time, as quite a few words (usually proper nouns) will only be seen a few times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll just remove any word that appears only once, so only keep words that were observed more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_v6(book, stopwords):\n",
    "    stemmer=nltk.stem.SnowballStemmer('english') #changed order of some steps to improve processing\n",
    "    \n",
    "    low=book.lower()\n",
    "    lines=low.split(\"\\n\")\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        words.extend(line.strip().split(\" \"))\n",
    "           # Skip stopwords OR words with length under 2\n",
    "    new_words=[]\n",
    "    tokens=[]\n",
    "    new_tokens=[]\n",
    "    stem_words=[]\n",
    "\n",
    "    for word in words:\n",
    "###Use endswith() function\n",
    "\n",
    "        if word.endswith(\"!\") or word.endswith(\".\") or word.endswith(\",\") or word.endswith(\":\") or word.endswith(\";\") or word.endswith(\"?\"):\n",
    "    \n",
    "            new_tokens.append(word[:-1])\n",
    "        else:\n",
    "            new_tokens.append(word)\n",
    "                \n",
    "    for word in new_tokens:\n",
    "        if word in stopwords:\n",
    "            pass\n",
    "        else:\n",
    "            tokens.append(word)\n",
    "            \n",
    "    for word in tokens:\n",
    "        if len(word) <= 2:   #lots of MR so i did less than or equal to rather than less than \n",
    "            pass\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "        \n",
    "    for word in new_words:\n",
    "        stemmed = stemmer.stem(word)  # This is all that is required--nltk does the rest!\n",
    "        stem_words.append(stemmed)\n",
    "\n",
    "    \n",
    "    word_counts = count(stem_words)\n",
    "    \n",
    "    ### NEW CODE HERE! It drops any keys whose values \n",
    "    ### (word counts) are only 1.\n",
    "    trimmed_counts = {}\n",
    "    for k, v in word_counts.items():\n",
    "        if v > 1: # Perform the check that the value (count) is larger than 1\n",
    "            trimmed_counts[k] = v\n",
    "    return trimmed_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One final check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'alice': 1502 unique words; most common word 'said' appeared 456 times.\n",
      "'pride': 3002 unique words; most common word 'elizabeth' appeared 625 times.\n",
      "'frank': 3073 unique words; most common word 'feel' appeared 152 times.\n",
      "'bwulf': 2543 unique words; most common word 'beowulf' appeared 192 times.\n",
      "'holmes': 3822 unique words; most common word 'said' appeared 484 times.\n",
      "'tom': 3419 unique words; most common word 'tom' appeared 705 times.\n",
      "'finn': 3428 unique words; most common word 'say' appeared 847 times.\n"
     ]
    }
   ],
   "source": [
    "counts = {}\n",
    "for k, v in books.items():\n",
    "    counts[k] = preprocess_v6(v, stopwords)\n",
    "    \n",
    "print_results(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The most common words and their counts haven't changed, but hopefully you can see there's a big difference in the number of unique words after dropping any word that only appeared once!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " - *Frankenstein*: 5256 to 3053\n",
    " - *Beowulf*: 6871 to 2543\n",
    " - *Alice in Wonderland*: 3174 to 1448\n",
    " - *Tom Sawyer*: 7596 to 3459\n",
    " - *Pride and Prejudice*: 5848 to 3059\n",
    " - *Sherlock Holmes*: 8004 to 3822\n",
    " - *Huckleberry Finn*: 8070 to 3521"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing and ignoring words that only appeared once, we eliminated a *ton* of words from the word count dictionaries of each of these books. Since the words only appeared once they were unlikely to contribute anything meaningful to any analysis we did; at the same time, we have significantly fewer words to worry about. Win-win!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we have the document vectors in bag of words format, fully preprocessed, we can do some analysis, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**What is it we're really hoping these word counts tell us?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What words are important in the book. The top 20 words could tell us a summary of a story and help match search inquiries to the book. it can also match a book by how often the word youre looking for is said with a high number of counts being more imortant or more relevant to your search. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
